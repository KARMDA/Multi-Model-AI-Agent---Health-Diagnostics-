# langgraph_flow_example.yaml
# Template: adapt to your LangGraph deployment syntax
nodes:
  - id: input
    type: http_input
    description: "Receives a JSON payload { 'row': {...} }"

  - id: mistral_node
    type: llm
    config:
      model: mistral
      call_endpoint: http://host.docker.internal:11434/api/generate
      prompt_template: |
        {{ row | to_prompt }}   # adapt templating to your LangGraph environment
    next: verifier_node

  - id: verifier_node
    type: function
    config:
      module: verifier
      function: verify_result
    next: sink_node

  - id: sink_node
    type: http_output
    config:
      status_on_ok: 200
